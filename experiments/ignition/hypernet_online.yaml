#Experiment configuration file.

#PT Lightning trainer arguments.
#Documentation: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
#Comment out options you aren't using
train:
  accelerator: "cpu" #str e.g. "gpu"
  devices: "auto" #int
  # num_nodes: #int
  # strategy: #str
  # precision: #str
  enable_checkpointing: True #bool
  check_val_every_n_epoch: 1 #int
  default_root_dir: "./lightning_logs" #str
  logger: True #bool
  log_every_n_steps: 1 #int
  enable_progress_bar: True #bool
  # profiler: #str
  limit_val_batches: 0 #bool
  max_epochs: 1 #int
  # max_steps: #int

#Model arguments
model:
  # ckpt_path: "lightning_logs/ignition/batching/time/version_1/checkpoints/best-epoch=89.ckpt" #str, only use this if you want to continue training model from checkpoint
  inr_type: "siren"
  output_activation: "Identity"
  hidden_features: 16
  blocks: 3
  hyper_hidden_features: 16
  loss_fn: "R3Loss"
  learning_rate: 0.0001 #float
  scheduler: False

#PT LightningDataModule arguments
data:
  spatial_dim: 2 #int
  points_path: "points.npy" #str
  features_path: "features_10.npy" #str
  gradients: False #bool
  sample_factor: 0.10
  batch_size: 1 #int
  channels: [0,1] #list[int]
  normalize: False #bool
  split: 1.0 #float [0,1]
  shuffle: False #bool
  buffer:
    # full:
    #   size: 16 #int
    #   cycles: 1 #int
    #   batch_size: 16 #int
    #   slide: True #bool
    # coarse:
    #   size: -1 #int
    #   cycles: 1 #int
    #   batch_size: 64 #int
    #   slide: True #bool
    full:
      window_size: 1
      cycles: 500
    coarse:
      step: 1
      cycles: 500
      batch_size: 10

#Miscellaneous arguments
misc:
  early_stopping: False #bool
  make_gif: True #bool
  make_error_plots: True
  compute_stats: True #bool
  export: True
  finetune: False