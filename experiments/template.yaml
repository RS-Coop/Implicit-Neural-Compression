#Experiment configuration file.

#PT Lightning trainer arguments.
#Documentation: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
#Comment out options you aren't using
train:
  accelerator: "auto" #str e.g. "gpu"
  devices: "auto" #int
  # num_nodes: #int
  # strategy: #str
  # precision: #str
  enable_checkpointing: #bool
  check_val_every_n_epoch: #int
  default_root_dir: #str
  logger: #bool
  log_every_n_steps: #int
  enable_progress_bar: #bool
  # profiler: #str
  limit_val_batches: #int
  max_epochs: #int
  # max_steps: #int

#Model arguments
model:
  # ckpt_path: #str, only use this if you want to continue training model from checkpoint
  output_activation: #string, torch.nn
  hypernet_kwargs:
    hidden_features: #int
    blocks: #int
  inr_kwargs:
    hidden_features: #int
    blocks: #int
  loss_fn: #string, torch.nn or custom
  learning_rate: #float
  scheduler: #bool
  sketch_type: #string, fjlt or subsample
  cycles: #int
  loss_threshold: #float

#PT LightningDataModule arguments
data:
  spatial_dim: #int
  data_dir: #str
  points_path: #str
  features_path: #str
  gradients: #bool
  sample_factor: #float
  sketch_type: #string, fjlt or subsample
  batch_size: #int
  channels: #list[int]
  normalize: #bool
  split: #float [0,1]
  shuffle: #bool
  buffer:
    full:
      size: #int
      cycles: #int
      batch_size: #int
      step: #int
    coarse:
      size: #int
      cycles: #int
      batch_size: #int
      step: #int
      delay: #bool

#Miscellaneous arguments
misc:
  early_stopping: #bool
  make_gif: #bool
  compute_stats: #bool
  make_error_plots: #bool
  export: #bool
  finetune: #bool
  export_hnet: #bool
